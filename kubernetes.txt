Kubernetes (K8s) - это система управления контейнерами
- автоматическое развертывание приложений в контейнерах на серверах
- распределение нагрузки по серверам имеющимся в кластере кубера
- авто масштабирование, если нагрузка возрастает
- мониторинг и проверка работоспособности (health check), замена неработающих контейнеров

Среды запуска контейнеров (container runtime environment)
- docker engine
- CRI-O
- containerd

POD - самый маленький элемент, внутри которого запускаются контейнеры. K8s создает поды на различных серверах.
Под состоит из одного или нескольких контейнеров, общего тома, общий IP адрес для контейнеров.
Рекомендуется схема: 1 контейнер - 1 под
Но если какой то контейнер (микросервис) тесно связан с другим, то можно создавать несколько контейнеров внутри одного пода

    Кластер кубернетис

Состоит из улов (nodes) - каждый узел это один сервер (виртуальный, реальный). Они могут находится в разных частях мира, но логически принадлежать одному кластеру кубера.
В каждой node запускаются несколько pods. А внутри подов - уже контейнер или контейнеры

У каждого node есть роль:
- главный узел (он только контролирует все узлы)
- рабочий узел (нвгрзка распределяется между ними)
Можно настраивать резервирование для главного узла (главный и резервный узел)

    Сервисы, которые запускаются на узлах в кластере

На всех узлах есть сервисы:
- kubelet (коммуникация между nodes в рамках кластера. Он взаимодействует с API server на главной ноде)
- kube-proxy (отвечает за сеть)
- container runtime (отвечает за создание и контроль контейнеров на каждом узле)
 Кроме того на главном узле есть доп сервисы:
- API server 
- scheduler (отвечает за планирование и распределение нагрузки между узлами в рамках кластера) н-р: я могу сказать на гласном узле, что хочу развернуть приложение на 10 подах и уже он решает где их разворачивать и как распределять нагрузку
- kube controller manager (контролирует все поды в рамках кластера)
- cloud controller manager (взаисодействует с провайдерским менеджером, который предоставляет услугу создания облачного кластера)
- etcd (сохранение логов и инфы о кластере в матер ноде)
Внутри мастер ноды есть DNS сервис и поэтому внутри кластера можно ваимодействовать с помощтю имен

    Упаравление кластером с помощью kubectl

Чтобы управлять всем класетром нужна утилита - kubectl (в CLI). Есть графическая надтройка - K8s dashboard
Это клиент для управления локальным или удаленным кластером K8s
Можно со своего компа управлять кластером, который в облаке
kubectl связывается через API с API server по https
Его надо установить на компе

https://kubernetes.io/ru
https://kubernetes.io/docs/tasks/tools/ - для устаноки kubectl и minicube

Установка kubectl:
https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/

Лучше выполнять команды в GIT Bash
Лучше устанавливать через curl
kubectl version --client

Установка minicube:
https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download
Лучше усьановить через PowerShell и затем через него же добавить в $PATH (в режиме админа)
minikube version

minikube start - для запуска кластера. Он автоматически найдет докер, если он есть и начнет скачивать образ и запускать контейнер minikube (этот контейнер и будет являться кластером в нашей среде обучения и уже в нем мы будем поднимать поды, как бы контейнер в контейнере)
можно выбрать где поднимать minikube:
minikube start --driver=название
minikube start --cpus=4 --memory=8gb --disk-size=4gb
minikube start --cpus=4 --memory=4gb --driver=vmware - при создании вирт машины она не появляется в графическрм интерфесе, однако для того чтобы появилась надо найти ее в соответствующей папке, путь можно увидеть пвыполнив команду при включенной вирт машине: 
    vmrun list


minikube status - посмотреть статус кластера
    minikube
    type: Control Plane
    host: Running
    kubelet: Running
    apiserver: Running
    kubeconfig: Configured


alias k=kubectl - делаем алиас для удобства (можно поместить его в .bashrc в домашнюю директорию для сохранения навсегда)
alias mk="minikube kubectl --"

k cluster-info
    Kubernetes control plane is running at https://127.0.0.1:55210
    CoreDNS is running at https://127.0.0.1:55210/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

    To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

k get nodes
    NAME       STATUS   ROLES           AGE   VERSION
    minikube   Ready    control-plane   40m   v1.30.0

k get pods
    No resources found in default namespace. - подов нет ИМЕННО в дефолтном неймспейсе, однако есть и другие неймспейсы

k get namespaces
    NAME              STATUS   AGE
    default           Active   41m
    kube-node-lease   Active   41m
    kube-public       Active   41m
    kube-system       Active   41m


k get pods --namespace=kube-system
    NAME                               READY   STATUS    RESTARTS      AGE
    coredns-7db6d8ff4d-c84l8           1/1     Running   0             42m
    etcd-minikube                      1/1     Running   0             43m
    kube-apiserver-minikube            1/1     Running   0             43m
    kube-controller-manager-minikube   1/1     Running   0             43m
    kube-proxy-t8h49                   1/1     Running   0             42m
    kube-scheduler-minikube            1/1     Running   0             43m
    storage-provisioner                1/1     Running   1 (42m ago)   43m

minikube ip - посмотреть ip кластера (у нас есть тока одна нода - minikube)
    192.168.49.2

minikube ssh - для ssh подключеyия к ноде, чтобы не заморачиваься с портами так как нода в docker
Уже подключившись по ssh можно посмотреть какие контейнеры подняты в данной ноде:
docker ps
    CONTAINER ID   IMAGE                       COMMAND                  CREATED             STATUS             PORTS     NAMES
    cd4a12a3572c   6e38f40d628d                "/storage-provisioner"   About an hour ago   Up About an hour             k8s_storage-provisioner_storage-provisioner_kube-system_86c9629f-0787-4893-af93-88298ef2747c_1
    64b3999a238a   cbb01a7bd410                "/coredns -conf /etc…"   About an hour ago   Up About an hour             k8s_coredns_coredns-7db6d8ff4d-c84l8_kube-system_48bf52cb-c540-420d-9348-5ddbb4cb5004_0
    fa2bc70eb04c   a0bf559e280c                "/usr/local/bin/kube…"   About an hour ago   Up About an hour             k8s_kube-proxy_kube-proxy-t8h49_kube-system_53ec8c30-c56c-4960-833b-d53550429cb8_0
    cc1e97abb432   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_coredns-7db6d8ff4d-c84l8_kube-system_48bf52cb-c540-420d-9348-5ddbb4cb5004_0
    bd06669bdb5e   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_kube-proxy-t8h49_kube-system_53ec8c30-c56c-4960-833b-d53550429cb8_0
    7696575c92b2   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_storage-provisioner_kube-system_86c9629f-0787-4893-af93-88298ef2747c_0
    d2b78ce39009   c7aad43836fa                "kube-controller-man…"   About an hour ago   Up About an hour             k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_7fd44e8d11c3e0ffe6b1825e2a1f2270_0
    a0f49eff5b3a   3861cfcd7c04                "etcd --advertise-cl…"   About an hour ago   Up About an hour             k8s_etcd_etcd-minikube_kube-system_063d6b9688927e601f52fd818d1305c5_0
    f1cd096662cb   c42f13656d0b                "kube-apiserver --ad…"   About an hour ago   Up About an hour             k8s_kube-apiserver_kube-apiserver-minikube_kube-system_3c555f828409b009ebee39fdbedfcac0_0
    f378acbc66fa   259c8277fcbb                "kube-scheduler --au…"   About an hour ago   Up About an hour             k8s_kube-scheduler_kube-scheduler-minikube_kube-system_f9c8e1d0d74b1727abdb4b4a31d3a7c1_0
    939cf8dacbaa   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_etcd-minikube_kube-system_063d6b9688927e601f52fd818d1305c5_0
    f836626abf4d   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_kube-controller-manager-minikube_kube-system_7fd44e8d11c3e0ffe6b1825e2a1f2270_0
    d6073edbe64a   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_kube-apiserver-minikube_kube-system_3c555f828409b009ebee39fdbedfcac0_0
    2df06e9e1448   registry.k8s.io/pause:3.9   "/pause"                 About an hour ago   Up About an hour             k8s_POD_kube-scheduler-minikube_kube-system_f9c8e1d0d74b1727abdb4b4a31d3a7c1_0

    Создание пода в кластере

k get pods - убеждаемся что нет подов в пространстве имен по умолчанию
    No resources found in default namespace.

k run my-nginx-pod --image=nginx - пишем название пода и образ, из которого создавать контейнер, который будет запущен внутри пода
    pod/my-nginx-pod created

k get pods
    NAME           READY   STATUS    RESTARTS   AGE
    my-nginx-pod   1/1     Running   0          61s

Данный под пингуется только с ноды, где он поднят, поэтому ужно зайт на под

k describe pod my-nginx-pod - полная инфа по поду

docker@minikube:~$ docker ps | grep nginx
    994719e380b3   nginx                       "/docker-entrypoint.…"   18 minutes ago   Up 18 minutes             k8s_my-nginx-pod_my-nginx-pod_default_c999d054-23c0-46d0-a00f-72a9f8baf995_0
    29bd280e434c   registry.k8s.io/pause:3.9   "/pause"                 18 minutes ago   Up 18 minutes             k8s_POD_my-nginx-pod_default_c999d054-23c0-46d0-a00f-72a9f8baf995_0
При создании конейнера создается второй контейнер - pause, который нужен чтобы зарезервировать ресурсы для определенного пода, а именно для my-nginx-pod, так как контейнер nginx может быть установлен или удален, но есурсы должны сохраниться

docker exec -it 994719e380b3 bash - находясь на поде minikube подключаемся внутрь контейнера

docker@minikube:~$ hostname -i
hostname -i
192.168.49.2 - таким образом все контейнеры внутри пода разделяют между собой ip адрес пода

root@my-nginx-pod:/# ls /usr/share/nginx/html
    ls /usr/share/nginx/html
    50x.html  index.html

k get pods -o wide - вывод с ip
    NAME           READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
    my-nginx-pod   1/1     Running   0          39m   10.244.0.3   minikube   <none>           <none>


k delete pod my-nginx-pod - удалить поду
    pod "my-nginx-pod" deleted

k get pods - то есть был удален под, а вместе с ним и контейнер
    No resources found in default namespace.

    Создание деплоймента

k create deployment my-nginx-deploy --image=nginx
    deployment.apps/my-nginx-deploy created

k get pods
    NAME                               READY   STATUS    RESTARTS   AGE
    my-nginx-deploy-7b69c488c9-8m25q   1/1     Running   0          3m6s - название пода основано на названии деплоймента (название деплоймента - id replicaSet - часть названия конкретного пода)

k describe deploy my-nginx-deploy - инфа о деплойменте
Таким образом мы создали деплоймент, в каждом дерлойметне был создан ReplicaSet (это типа шаблона) и каждый под считается репликой (то есть копией), и в поде запускается контейнер

    Увеличение количества подов в деплойменте

k scale deploy my-nginx-deploy --replicas=5
    deployment.apps/my-nginx-deploy scaled

k get deploy
    NAME              READY   UP-TO-DATE   AVAILABLE   AGE
    my-nginx-deploy   3/3     3            3           32m

$ k get pods
NAME                               READY   STATUS    RESTARTS   AGE
my-nginx-deploy-7b69c488c9-64vxd   1/1     Running   0          90s
my-nginx-deploy-7b69c488c9-8m25q   1/1     Running   0          33m
my-nginx-deploy-7b69c488c9-cr6vl   1/1     Running   0          90s

    Сервисы, создание сервисов

k get services - посмотерть существующие сервисы

1) Создание сервиса ClusterIP

$ k expose deploy my-nginx-deploy --port=8080 --target-port=80 - создаем сервис ClusterIP для нашего деплоймента, можно подключатьсячерез внешний порт 8080, который будет перенаправлять трафик на поды на порт 80. IP адрес создается автоматически
    service/my-nginx-deploy exposed

$ k get services
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes        ClusterIP   10.96.0.1       <none>        443/TCP    17m - системный сервис
my-nginx-deploy   ClusterIP   10.102.133.98   <none>        8080/TCP   2m16s - сервис нашего деплоймента, этой ip доступен только внутри кластера!!!

$ k describe services my-nginx-deploy - просмотреть инфу о сервисе
Name:              my-nginx-deploy
Namespace:         default
Labels:            app=my-nginx-deploy
Annotations:       <none>
Selector:          app=my-nginx-deploy
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.102.133.98 
IPs:               10.102.133.98
Port:              <unset>  8080/TCP
TargetPort:        80/TCP
Endpoints:         10.244.0.4:80,10.244.0.5:80,10.244.0.6:80 + 2 more...
Session Affinity:  None
Events:            <none>
Таким образом при обращении на ip 10.102.133.98:8080 идет обращение к одному из подов (Endpoints), то есть выполняется балансировка нагрузки между ними

Пробуем подключится:
minikube ssh - заходим на кластер
curl 10.102.133.98:8080 - открывается страница приветствия сайта nginx (мы получили ответ от одного из подов в нашем деплойменте)
ВНИМАНИЕ! Этот ip доступен только внутри кластера кубернетис!!!

Но этот тип сервиса (Cluster ip) нужен если мы хотим чтобы с данным деплойментом взаимодействовали другие деплойменты и который не должен быть доступен снаружи, но если должен смотерть наружу, то нужно использовать NodePort или LoadBalancer

2) Создание сервиса NodePort

$ k delete svc my-nginx-deploy - удалим сервис ClusterIP
service "my-nginx-deploy" deleted

$ k expose deploy my-nginx-deploy --type=NodePort --port=8888 --target-port=80
service/my-nginx-deploy exposed

$ k get svc
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes        ClusterIP   10.96.0.1        <none>        443/TCP          40m
my-nginx-deploy   NodePort    10.110.148.117   <none>        8888:32329/TCP   42s


$ minikube ip - смотрим ip кластера
192.168.207.129

проверяем подключение:
curl 192.168.207.129:32329 или через браузер

$ k delete svc my-nginx-deploy
service "my-nginx-deploy" deleted

3) Создание сервиса LoadBalancer

$ k expose deploy my-nginx-deploy --type=LoadBalancer --port=9999 --target-port=80
service/my-nginx-deploy exposed

$ k get svc
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes        ClusterIP      10.96.0.1       <none>        443/TCP          61m
my-nginx-deploy   LoadBalancer   10.109.12.140   <pending>     9999:31284/TCP   83s

Теперь через внешний ip кластера мы можем подключится к сервису my-nginx-deploy



docker build . -t 01yura/k8s-web-hello-ru:latest -t 01yura/k8s-web-hello-ru:1.0.0 - создать образ из Dockerfile
docker push 01yura/k8s-web-hello-ru --all-tags - загрузить на докерхаб оба образа

k create deploy k8s-web-hello --image=01yura/k8s-web-hello-ru:latest - создаем деплоймент из этого контейнера

k expose deploy k8s-web-hello --type=LoadBalancer --port=3333 --target-port=3000

    Обновление приложения (Rolling update)

обновляем код
docker build . -t 01yura/k8s-web-hello-ru:latest -t 01yura/k8s-web-hello-ru:2.0.0 - делаем образы
docker push 01yura/k8s-web-hello-ru --all-tags - пушим образы на dockerhub

k set image deploy k8s-web-hello k8s-web-hello-ru=01yura/k8s-web-hello-ru:2.0.0 - указываем какой deployment обновляем, затем какой образ обновляем , затем указываем на какой образ мы хотим обновить

k rollout status deploy k8s-web-hello - проверяем статус выкатки

    YAML файлы для деплойментов и сервисов

- установить расширение для VScode - Kubernetes
- создать в VSCode YAML файл deployment.yaml (можно просто .yml) и service.yaml

k apply -f deployment.yaml - создаем деплоймент
k apply -f service.yaml - создаем сервис

$ k delete -f deployment.yaml -f sevice.yaml - чтобы удалить деплоймент и сервис, но не файлы!
deployment.apps "k8s-web-hello" deleted
service "k8s-web-hello" deleted

minikube dashboard - подключится к дэшборд

Если запустили minikube с containerd вот так:
minikube start --driver=vmware --container-runtime=containerd

то запущенные контейнеры внутри кластера можно посмотреть не командой docker ps, а командой
sudo ctr -n k8s.io containers list
Он кстати не создает контейнеры на паузе

k delete all --all - удалить все деплойменты и сервисы

                    Создания кластера k8s в AWS

Надо установить 3 утилиты:
- awscli - yнужен для аутентификации и запуска команд aws
https://awscli.amazonaws.com/AWSCLIV2.msi - скачиваем установщик
aws --version

- eksctl - для создания кластера
https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_amd64.zip - скачиваем и файл добавляем в $PATH
- kubectl - для управления кластером

Далее вводим 
aws configure и вводим все данные
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: region-code
Default output format [None]: text

Эти ключи можно создать в настрйоках юзера
Проверить настроилось ли подключение можно командой:
aws sts get-caller-identity 

Включите OIDC для кластера:
eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster number5 --approve
Обновите аддон вручную:
eksctl update addon --name vpc-cni --cluster number5 --region us-east-1 --force

eksctl delete cluster --name=number5 --region=us-east-1 - удалить кластер

                    Установка и создание кластера на голых серверах
1) На масте ноде

Проверяем включен ли swap, и если да, то отключаем: (и на мастер и на воркер нодах)
swapon --show
sudo swapoff -a
sudo nano /etc/fstab - комментируем соответствующую строку в fstab
swapon -a - эта команда перечитывает файл fstab и проверяет нет ли ошибок (иначе система не загрузится)

Проверяем есть ли файерволл и не блокирует ли он порты:
sudo systemctl status firewalld
sudo iptables -L
sudo ufw status


Установка containerd:
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common gpg

sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install -y containerd.io

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd


Установка kubeadm, kubelet и kubectl:
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl



Выполните следующую команду для временного включения ip_forward:
sudo sysctl -w net.ipv4.ip_forward=1

Чтобы это изменение сохранилось после перезагрузки, добавьте строку в файл /etc/sysctl.conf:
echo "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

Инициализация мастер-ноды:
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

После инициализации появится сообщение:         (надо выполнить одну из представленных команд для рута или пользователя(лучше ту которая для юзера))

    Your Kubernetes control-plane has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    Alternatively, if you are the root user, you can run:

    export KUBECONFIG=/etc/kubernetes/admin.conf

    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

    Then you can join any number of worker nodes by running the following on each as root:

    kubeadm join 192.168.0.57:6443 --token t7mqhd.pjk37z46rh1duck2 \
        --discovery-token-ca-cert-hash sha256:c2cd097be9d53e185734e86243c47f4e402aeb7226b1c94ad3ae9ab78794b13c

Установка сетевого плагина (например, Calico):
https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

k apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml

2) На каждой воркер ноде

Делаем все то же самое, но 
Kubectl уствнвливать не обязательно, так как он используется для управления кластерм через мастер ноду
Не надо устанавливать сетевой плагин

Ввод ноды в кластер:
kubeadm join 192.168.0.57:6443 --token t7mqhd.pjk37z46rh1duck2 \
        --discovery-token-ca-cert-hash sha256:c2cd097be9d53e185734e86243c47f4e402aeb7226b1c94ad3ae9ab78794b13c







